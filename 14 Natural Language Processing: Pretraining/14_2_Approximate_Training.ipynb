{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Due to the nature of the softmax operation we've to consider every single word in the dictionary. To reduce this computational complexity we use two approximations: negative sampling and hierarchical softmax.\n",
        "\n"
      ],
      "metadata": {
        "id": "VUfjHwkKK-3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative Sampling\n",
        "\n",
        "Given the context window of a center word $w_c$, the fact that any (context) word $w_o$ comes from this context window is considered as an event with the probability modeled by $\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)$, $\\sigma$ is the sigmoid activation function."
      ],
      "metadata": {
        "id": "Ou_INImdLjZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Softmax\n",
        "\n",
        "<img src=\"https://classic.d2l.ai/_images/hi-softmax.svg\" />\n",
        "\n",
        "$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right)$\n",
        "\n",
        "Time-Complexity: $\\mathcal{O}(\\text{log}_2|\\mathcal{V}|)$"
      ],
      "metadata": {
        "id": "30DxjK93MWaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiwVhOvqK1my"
      },
      "outputs": [],
      "source": []
    }
  ]
}