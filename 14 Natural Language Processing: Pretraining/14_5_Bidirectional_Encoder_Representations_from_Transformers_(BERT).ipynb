{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "word2vec assigns the same pretrained vector to the same word regardless of the context of the word.\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) encodes context bidirectionally.\n",
        "\n"
      ],
      "metadata": {
        "id": "_QXMYcOVCBMK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jWW0ivgB1K-"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.17.5"
      ],
      "metadata": {
        "id": "ZKnZpFRDCeER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-inline"
      ],
      "metadata": {
        "id": "myTd88LuCfc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "bVk51ifXCgxQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Representation\n",
        "\n",
        "Take single text as the input. The input can be a pair of text sequences.\n",
        "\n",
        "The BERT input sequence is the concatenation of the special classification token \"\\<cls\\>\", tokens of a text sequence, and the special separation token \"\\<sep\\>\". If input pairs are taken, then the BERT input sequence is the concatenation of \"\\<cls\\>\", tokens of the first text sequence, \"\\<sep\\>\", tokens of the second text sequence, and \"\\<sep\\>\".\n",
        "\n",
        "For text inputs, embedding $\\mathbf{e}_A$ is used, for pairs, embeddings $\\mathbf{e}_A$ and $\\mathbf{e}_B$ are used.\n",
        "\n",
        "BERT chooses the transformer encoder as its bidirectional architecture. Common in the transformer encoder, positional embeddings are added at every position of the BERT input sequence. \n",
        "\n",
        "However, different from the original transformer encoder, BERT uses learnable positional embeddings.\n",
        "\n",
        "<img src=\"https://classic.d2l.ai/_images/bert-input.svg\" />"
      ],
      "metadata": {
        "id": "MXcYxtMqCwWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following get_tokens_and_segments takes either one sentence or two sentences as the input, then returns tokens of the BERT input sequence and their corresponding segment IDs."
      ],
      "metadata": {
        "id": "qT8eUJNcGUQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
        "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
        "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
        "    # 0 and 1 are marking segment A and B, respectively\n",
        "    segments = [0] * (len(tokens_a) + 2)\n",
        "    if tokens_b is not None:\n",
        "        tokens += tokens_b + ['<sep>']\n",
        "        segments += [1] * (len(tokens_b) + 1)\n",
        "    return tokens, segments"
      ],
      "metadata": {
        "id": "OjgFwX1aCurg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following BERTEncoder class is similar to the TransformerEncoder class. Different from TransformerEncoder, BERTEncoder uses segment embeddings and learnable positional embeddings."
      ],
      "metadata": {
        "id": "OjuyHHJUGedn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTEncoder(nn.Module):\n",
        "    \"\"\"BERT encoder.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
        "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
        "                 **kwargs):\n",
        "        super(BERTEncoder, self).__init__(**kwargs)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers):\n",
        "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n",
        "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
        "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
        "        # In BERT, positional embeddings are learnable, thus we create a\n",
        "        # parameter of positional embeddings that are long enough\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
        "                                                      num_hiddens))\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens):\n",
        "        # Shape of `X` remains unchanged in the following code snippet:\n",
        "        # (batch size, max sequence length, `num_hiddens`)\n",
        "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
        "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
        "        for blk in self.blks:\n",
        "            X = blk(X, valid_lens)\n",
        "        return X"
      ],
      "metadata": {
        "id": "apM0r6hgGW-P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretraining Tasks\n",
        "\n",
        "The pretraining is composed of the following two tasks: masked language modeling and next sentence prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "T0Pg6-ncG9f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masked Language Modeling\n",
        "\n",
        "To encode context bidirectionally for representing each token, BERT randomly masks tokens and uses tokens from the bidirectional context to predict the masked tokens in a self-supervised fashion."
      ],
      "metadata": {
        "id": "Hueu4GWnIKoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskLM(nn.Module):\n",
        "    \"\"\"The masked language model task of BERT.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
        "        super(MaskLM, self).__init__(**kwargs)\n",
        "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.LayerNorm(num_hiddens),\n",
        "                                 nn.Linear(num_hiddens, vocab_size))\n",
        "\n",
        "    def forward(self, X, pred_positions):\n",
        "        num_pred_positions = pred_positions.shape[1]\n",
        "        pred_positions = pred_positions.reshape(-1)\n",
        "        batch_size = X.shape[0]\n",
        "        batch_idx = torch.arange(0, batch_size)\n",
        "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
        "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
        "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
        "        masked_X = X[batch_idx, pred_positions]\n",
        "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
        "        mlm_Y_hat = self.mlp(masked_X)\n",
        "        return mlm_Y_hat"
      ],
      "metadata": {
        "id": "Hd5kHQa5Gjx_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Next Sentence Prediction\n",
        "\n",
        "To help understand the relationship between two text sequences, BERT considers a binary classification task, next sentence prediction, in its pretraining. "
      ],
      "metadata": {
        "id": "NCMOuJqWIIPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NextSentencePred(nn.Module):\n",
        "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
        "    def __init__(self, num_inputs, **kwargs):\n",
        "        super(NextSentencePred, self).__init__(**kwargs)\n",
        "        self.output = nn.Linear(num_inputs, 2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # `X` shape: (batch size, `num_hiddens`)\n",
        "        return self.output(X)"
      ],
      "metadata": {
        "id": "WsyyQIrkIBPg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting All Things Together"
      ],
      "metadata": {
        "id": "uyKAOvLCJIhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(nn.Module):\n",
        "    \"\"\"The BERT model.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
        "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
        "                 hid_in_features=768, mlm_in_features=768,\n",
        "                 nsp_in_features=768):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
        "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
        "                    dropout, max_len=max_len, key_size=key_size,\n",
        "                    query_size=query_size, value_size=value_size)\n",
        "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
        "                                    nn.Tanh())\n",
        "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
        "        self.nsp = NextSentencePred(nsp_in_features)\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
        "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
        "        if pred_positions is not None:\n",
        "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
        "        else:\n",
        "            mlm_Y_hat = None\n",
        "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
        "        # 0 is the index of the '<cls>' token\n",
        "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
        "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
      ],
      "metadata": {
        "id": "tib631LxJGCo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emgnNIGXJNUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}