{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The technique of mapping words to real vectors is called word embedding. "
      ],
      "metadata": {
        "id": "4uoJHo5C6U9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Vectors Are a Bad Choice\n",
        "\n",
        "\n",
        "Suppose that the number of different words in the dictionary (the dictionary size) is $N$, and each word corresponds to a different integer (index) from $0$ to $N - 1$. To obtain the one-hot vector representation for any word with index $i$, we create a length-$N$ vector with all 0s and set the element at position $i$ to 1.\n",
        "\n",
        "For vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their cosine similarity is the cosine of the angle between them:\n",
        "\n",
        "$\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\in [-1, 1]$\n",
        "\n",
        "Since the cosine similarity between one-hot vectors of any two different words is 0, one-hot vectors cannot encode similarities among words."
      ],
      "metadata": {
        "id": "5yGAici56ZFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Supervised word2vec\n",
        "\n",
        "The word2vec tool contains two models, namely skip-gram and continuous bag of words (CBOW).\n",
        "\n",
        "For semantically meaningful representations, their training relies on conditional probabilities that can be viewed as predicting some words using some of their surrounding words in corpora.\n",
        "\n",
        "Since supervision comes from the data without labels, both skip-gram and continuous bag of words are self-supervised models."
      ],
      "metadata": {
        "id": "ffkqxGE7EA7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Skip-Gram Model\n",
        "\n",
        "The skip-gram model assumes that a word can be used to generate its surrounding words in a text sequence.\n",
        "\n",
        "Take the text sequence \"the\", \"man\", \"loves\", \"his\", \"son\" as an example. Let us choose \"loves\" as the center word and set the context window size to 2. \n",
        "\n",
        "The skip-gram model considers the conditional probability for generating the context words: \"the\", \"man\", \"his\", and \"son\", which are no more than 2 words away from the center word: $P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"})$\n",
        "\n",
        "Assume that the context words are independently generated given the center word (i.e., conditional independence). In this case, the above conditional probability can be rewritten as $P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"})$\n",
        "\n",
        "<img src=\"https://classic.d2l.ai/_images/skip-gram.svg\" />\n",
        "\n",
        "\n",
        "The conditional probability of generating any context word $w_o$ (with index $o$ in the dictionary) given the center word $w_c$ (with index $c$ in the dictionary) can be modeled by a softmax operation on vector dot products:\n",
        "\n",
        "$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)}$, for any word with index $i$ in the dictionary, denote by $\\mathbf{v}_i\\in\\mathbb{R}^d$ and $\\mathbf{u}_i\\in\\mathbb{R}^d$ its two vectors when used as a center word and a context word, respectively."
      ],
      "metadata": {
        "id": "vns3R9jUEQgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Minimizing the following loss function:\n",
        "\n",
        "$- \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}\\, P(w^{(t+j)} \\mid w^{(t)})$, where a text sequence of length $T$, where the word at time step $t$ is denoted as $w^{(t)}$ for context window size $m$."
      ],
      "metadata": {
        "id": "Xn9VCZcrGyw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Continuous Bag of Words (CBOW) Model\n",
        "\n",
        "Continuous bag of words model assumes that a center word is generated based on its surrounding context words in the text sequence.\n",
        "\n",
        "<img src=\"https://classic.d2l.ai/_images/cbow.svg\" />\n",
        "\n",
        "$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}}) \\right)}$"
      ],
      "metadata": {
        "id": "mm0QAAXJJXfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Loss function:\n",
        "\n",
        "$-\\sum_{t=1}^T  \\text{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$"
      ],
      "metadata": {
        "id": "InqUP0eEKHEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc4gxeWc5xhB"
      },
      "outputs": [],
      "source": []
    }
  ]
}