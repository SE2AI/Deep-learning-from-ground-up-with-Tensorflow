{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27595013",
   "metadata": {},
   "source": [
    "### Covariate Shift\n",
    "\n",
    "- Distributions can change over time\n",
    "\n",
    "![pdf-defin-nls.png](../images/pdf-defin-nls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa77f2",
   "metadata": {},
   "source": [
    "### Label Shift\n",
    "\n",
    "Predict diagnoses given their symptoms can become predict symptoms given disease\n",
    "\n",
    "$P(y \\mid \\mathbf{x})$ becomes $P(\\mathbf{x} \\mid y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214e4f2",
   "metadata": {},
   "source": [
    "### Concept Shift\n",
    "\n",
    "Favourite beverages can be different for different locations. Essentially distributions, $P(y \\mid \\mathbf{x})$ can be different for different locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110899b",
   "metadata": {},
   "source": [
    "### Correction of Distribution Shift\n",
    "\n",
    "- Empirical Risk and Risk: The empirical risk is an average loss over the training data. Risk is average loss over the entire population. \n",
    "- Since we know how to optimize the loss, $\\mathop{\\mathrm{minimize}}_f \\frac{1}{n} \\sum_{i=1}^n l(f(\\mathbf{x}_i), y_i)$, we can calculate empirical risk as $E_{p(\\mathbf{x}, y)} [l(f(\\mathbf{x}), y)] = \\int\\int l(f(\\mathbf{x}), y) p(\\mathbf{x}, y) \\;d\\mathbf{x}dy$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab0ed8",
   "metadata": {},
   "source": [
    "### Covariate Shift Correction\n",
    "\n",
    "- If our data has been drawn from source-distribution $\\large Q(y \\mid \\mathbf{x})$, instead of target-distribution $P(y \\mid \\mathbf{x})$, following dependency assumption, $p(y \\mid \\mathbf{x}) = q(y \\mid \\mathbf{x})$, so $\\begin{aligned}\n",
    "\\int\\int l(f(\\mathbf{x}), y) p(y \\mid \\mathbf{x})p(\\mathbf{x}) \\;d\\mathbf{x}dy =\n",
    "\\int\\int l(f(\\mathbf{x}), y) q(y \\mid \\mathbf{x})q(\\mathbf{x})\\frac{p(\\mathbf{x})}{q(\\mathbf{x})} \\;d\\mathbf{x}dy\n",
    "\\end{aligned}$ and we can find $\\large \\beta_i \\stackrel{\\mathrm{def}}{=} \\frac{p(\\mathbf{x}_i)}{q(\\mathbf{x}_i)}$ and plug it into $\\mathop{\\mathrm{minimize}}_f \\frac{1}{n} \\sum_{i=1}^n \\beta_i l(f(\\mathbf{x}_i), y_i) \\newline$\n",
    "\n",
    "- Finding $\\large \\beta_i$ $\\newline$\n",
    " - Now denote by $z$ labels that are $1$ for data drawn from $p$ and $-1$ for data drawn from $q$ $\\newline$.\n",
    " - $\\large P(z=1 \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})} \\text{ and hence } \\frac{P(z=1 \\mid \\mathbf{x})}{P(z=-1 \\mid \\mathbf{x})} = \\frac{p(\\mathbf{x})}{q(\\mathbf{x})}$ $\\newline$\n",
    " - If we use a logistic regression approach, where $\\large P(z=1 \\mid \\mathbf{x})=\\frac{1}{1+\\exp(-h(\\mathbf{x}))}$, ($h$ is a parameterized function) $\\newline$\n",
    " - $\\large P(z=-1 \\mid \\mathbf{x}) = 1- P(z=1 \\mid \\mathbf{x}) = 1 - \\frac{1}{1+\\exp(-h(\\mathbf{x}))} = \\frac{\\exp(-h(\\mathbf{x})}{1+\\exp(-h(\\mathbf{x}))}$ $\\newline$\n",
    " \n",
    " - $\\large \\beta_i  = \\exp(h(\\mathbf{x}_i))$ $\\newline$\n",
    " \n",
    "- Finding $\\large h$ $\\newline$\n",
    " - Generate a binary-classification training set: $\\{(\\mathbf{x}_1, -1), \\ldots, (\\mathbf{x}_n, -1), (\\mathbf{u}_1, 1), \\ldots, (\\mathbf{u}_m, 1)\\}$ $\\newline$\n",
    " - Train a binary classifier using logistic regression to get function $h$ $\\newline$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0ee86",
   "metadata": {},
   "source": [
    "### Label Shift Correction\n",
    "\n",
    "- $q(y) \\neq p(y)$ but $q(\\mathbf{x} \\mid y)=p(\\mathbf{x} \\mid y)$ $\\newline$\n",
    "\n",
    "- $\\begin{aligned}\n",
    "\\int\\int l(f(\\mathbf{x}), y) p(\\mathbf{x} \\mid y)p(y) \\;d\\mathbf{x}dy =\n",
    "\\int\\int l(f(\\mathbf{x}), y) q(\\mathbf{x} \\mid y)q(y)\\frac{p(y)}{q(y)} \\;d\\mathbf{x}dy.\n",
    "\\end{aligned}$ $\\newline$\n",
    "\n",
    "- $\\beta_i \\stackrel{\\mathrm{def}}{=} \\frac{p(y_i)}{q(y_i)}$ $\\newline$\n",
    "\n",
    "- Take the model trained on train data and compute its confusion matrix, $C$ using the validation set $\\newline$\n",
    "\n",
    "- Average all of our models predictions at test time to find, $\\mu(\\hat{\\mathbf{y}}) \\in \\mathbb{R}^k$ $\\newline$\n",
    "\n",
    "- Solve $\\mathbf{C} p(\\mathbf{y}) = \\mu(\\hat{\\mathbf{y}})$ $\\newline$\n",
    "\n",
    "- Simarly find $q(y)$ to get $\\beta_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3979e2",
   "metadata": {},
   "source": [
    "### Concept Shift Correction\n",
    "\n",
    "- We use the existing network weights and simply perform a few update steps with the new data rather than training from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
