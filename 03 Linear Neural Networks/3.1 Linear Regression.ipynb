{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8bdbdb",
   "metadata": {},
   "source": [
    "- Assumptions of Linear Regression\n",
    "    - Relationship between the independent variables  $x$  and the dependent variable  $y$  is linear, i.e., $y$  can be expressed as a weighted sum of the elements in $x$ , given some noise on the observations\n",
    "    - Any noise $\\epsilon$ is well-behaved (following a Gaussian distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bec4d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "1. Training Set\n",
    "2. Validation Set\n",
    "3. Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af00cf",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "Let $n$ denotes the number of examples in our dataset. We index the data examples by $i$, denoting each input as $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$ and the corresponding label as $y^{(i)}$\n",
    "\n",
    "Prediction: $\\hat{y} = w_1  x_1 + ... + w_d  x_d + b$, where $\\mathbf{w}$ is weights vector, $\\mathbf{x}$ is feature Vector and $b$ is bias or $y$-intercept.\n",
    "\n",
    "Compact form:\n",
    "\n",
    "$\\hat{y} = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "\n",
    "For a collection of features $\\mathbf{X}$\n",
    "\n",
    "${\\hat{\\mathbf{y}}} = \\mathbf{w} \\cdot \\mathbf{X} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f595720",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "- measure of how close is $\\hat{y}^{(i)}$ to $y^{(i)}$\n",
    "\n",
    "squared error \n",
    "\n",
    "$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$\n",
    "\n",
    "To measure the quality of a model on the entire dataset of  $n$  examples, we take the average loss:\n",
    "\n",
    "$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2$\n",
    "\n",
    "When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$), that minimizes $L(\\mathbf{w}, b)$:\n",
    "\n",
    "$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ca079",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Taking the partial derivative of $L(\\mathbf{w}, b)$ over the entire dataset, w.r.t. $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f8ad5",
   "metadata": {},
   "source": [
    "### Minibatch Stochastic Gradient Descent\n",
    "\n",
    "1. Initialize the values of the model parameters: $w$ and $b$, often randomly\n",
    "2. In each iteration, we randomly sample a minibatch $\\mathcal{B}$ from our dataset\n",
    "3. Taking the partial derivative of $L(\\mathbf{w}, b)$, w.r.t. $w$ and $b$\n",
    "4. Update $w, b$:\n",
    "$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)$, where $\\eta$ stands for learning rate, $|\\mathcal{B}|$ is cardinality\n",
    "\n",
    "$\\eta$ and $|\\mathcal{B}|$ are called $\\textit{hyperparameters}$, are not updated in the training loop like $w$ and $b$, they're typically adjusted based on results on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffffb34",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Once we find $\\mathbf{w}^*, b^*$ we can make predictions. Let $\\mathbf{x_{val}}$ denotes our Validation Set, then $\\hat y = \\mathbf{w}^* \\cdot x_{val} + b^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276ef2a",
   "metadata": {},
   "source": [
    "### Neuron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6ddd50a",
   "metadata": {},
   "source": [
    "![neuron.svg](../images/neuron.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76ceb6",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "168dd791",
   "metadata": {},
   "source": [
    "![singleneuron.svg](../images/singleneuron.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
