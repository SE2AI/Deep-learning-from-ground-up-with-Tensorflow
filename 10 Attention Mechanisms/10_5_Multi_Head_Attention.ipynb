{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.5. Multi-Head Attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Download Template"
      ],
      "metadata": {
        "id": "5oc8qgGfo4i8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4tGjLsBowaJ",
        "outputId": "f4dd6ba0-7171-4cda-a1da-9391e82885df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-24 00:06:42--  https://gist.githubusercontent.com/b1nch3f/b702d2ebfd2c751aa21b68c7425e0d4e/raw/c77225291924a8d2c33c210d74f2744210b6b716/d2l.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59707 (58K) [text/plain]\n",
            "Saving to: ‘d2l.py’\n",
            "\n",
            "\rd2l.py                0%[                    ]       0  --.-KB/s               \rd2l.py              100%[===================>]  58.31K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-07-24 00:06:42 (5.05 MB/s) - ‘d2l.py’ saved [59707/59707]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/b1nch3f/b702d2ebfd2c751aa21b68c7425e0d4e/raw/c77225291924a8d2c33c210d74f2744210b6b716/d2l.py -O d2l.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of performing a single attention pooling, queries, keys, and values can be transformed with $h$ independently learned linear projections. Then these $h$ projected queries, keys, and values are fed into attention pooling in parallel. In the end, $h$ attention pooling outputs are concatenated and transformed with another learned linear projection to produce the final output.\n",
        "\n",
        "<img src=\"https://classic.d2l.ai/_images/multi-head-attention.svg\" />"
      ],
      "metadata": {
        "id": "fgXdCJxdgoBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Given a query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$, a key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$, and a value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$, each attention head  ($i = 1, \\ldots, h$) is computed as\n",
        "\n",
        "$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v}$\n",
        "\n",
        "where learnable parameters $\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$\n",
        ", $\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\n",
        " and $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$\n",
        ", and $f$ is attention pooling, such as additive attention and scaled dot-product attention. The multi-head attention output is another linear transformation via learnable parameters $\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$ of the concatenation of $h$ heads:\n",
        "\n",
        "$\\begin{split}\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.\\end{split}$"
      ],
      "metadata": {
        "id": "OC6wMN0og7vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import d2l"
      ],
      "metadata": {
        "id": "mQzVzR2Ds0dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "K_TijDrahz_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
        "                 num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = d2l.DotProductAttention(dropout)\n",
        "        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
        "        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
        "        self.W_v = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
        "        self.W_o = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
        "\n",
        "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
        "        # Shape of `queries`, `keys`, or `values`:\n",
        "        # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`)\n",
        "        # Shape of `valid_lens`:\n",
        "        # (`batch_size`,) or (`batch_size`, no. of queries)\n",
        "        # After transposing, shape of output `queries`, `keys`, or `values`:\n",
        "        # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n",
        "        # `num_hiddens` / `num_heads`)\n",
        "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
        "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
        "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            # On axis 0, copy the first item (scalar or vector) for\n",
        "            # `num_heads` times, then copy the next item, and so on\n",
        "            valid_lens = tf.repeat(valid_lens, repeats=self.num_heads, axis=0)\n",
        "\n",
        "        # Shape of `output`: (`batch_size` * `num_heads`, no. of queries, `num_hiddens` / `num_heads`)\n",
        "        output = self.attention(queries, keys, values, valid_lens, **kwargs)\n",
        "\n",
        "        # Shape of `output_concat`: (`batch_size`, no. of queries, `num_hiddens`)\n",
        "        output_concat = transpose_output(output, self.num_heads)\n",
        "        return self.W_o(output_concat)"
      ],
      "metadata": {
        "id": "B0ThIx4Dhv1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose_qkv(X, num_heads):\n",
        "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
        "    # Shape of input `X`:\n",
        "    # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`).\n",
        "    # Shape of output `X`:\n",
        "    # (`batch_size`, no. of queries or key-value pairs, `num_heads`,\n",
        "    # `num_hiddens` / `num_heads`)\n",
        "    X = tf.reshape(X, shape=(X.shape[0], X.shape[1], num_heads, -1))\n",
        "\n",
        "    # Shape of output `X`:\n",
        "    # (`batch_size`, `num_heads`, no. of queries or key-value pairs,\n",
        "    # `num_hiddens` / `num_heads`)\n",
        "    X = tf.transpose(X, perm=(0, 2, 1, 3))\n",
        "\n",
        "    # Shape of `output`:\n",
        "    # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n",
        "    # `num_hiddens` / `num_heads`)\n",
        "    return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3]))\n",
        "\n",
        "\n",
        "def transpose_output(X, num_heads):\n",
        "    \"\"\"Reverse the operation of `transpose_qkv`.\"\"\"\n",
        "    X = tf.reshape(X, shape=(-1, num_heads, X.shape[1], X.shape[2]))\n",
        "    X = tf.transpose(X, perm=(0, 2, 1, 3))\n",
        "    return tf.reshape(X, shape=(X.shape[0], X.shape[1], -1))"
      ],
      "metadata": {
        "id": "qCuo1dYSiGKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens, num_heads = 100, 5\n",
        "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
        "                               num_hiddens, num_heads, 0.5)\n",
        "\n",
        "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, tf.constant([3, 2])\n",
        "X = tf.ones((batch_size, num_queries, num_hiddens))\n",
        "Y = tf.ones((batch_size, num_kvpairs, num_hiddens))\n",
        "attention(X, Y, Y, valid_lens, training=False).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDhWy0d8iNkS",
        "outputId": "3fbee320-f4ad-4d8b-dfa8-0a39cd654f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 4, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1HQLpIgCiPXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}